{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2fa88e",
   "metadata": {},
   "source": [
    "# Learning closeness centrality using deep learning model and features extracted via the RNDFC matrix rep. of nodes\n",
    "\n",
    "This notebook is completes the discussion in Example 7.4 in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f52305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygraph import MyGraph\n",
    "from helpers import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11899144",
   "metadata": {},
   "source": [
    "## Loading the dataset from numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40115369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23340831])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# closeness centrality as the target or label data\n",
    "target_closeness = np.load(\"fb_media_pages_closeness.npy\", mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\n",
    "target_closeness[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e44f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the RNDFC matrix as the feature set\n",
    "matrix = np.load(\"fb_media_pages_RNDFC_matrix_r1_3_sta1_max70_rad3.npy\", mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8797e768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27917, 1), (27917, 4, 23))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_closeness.shape, matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a63416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  4.,  2.,  4.,  2.,  1.,  1.,  3.,  2.,  1.,  3.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 2.,  7., 11., 28., 33., 31., 35., 18., 34., 30., 24., 17.,  9.,\n",
       "        13.,  7.,  2.,  9.,  2.,  5.,  5.,  4.,  2.,  2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da472d",
   "metadata": {},
   "source": [
    "# The p-aggregation of the RCDF matrix\n",
    "\n",
    "To apply this aggregation we just need to multiply the following row matrix from left to the RCDF matrix:\n",
    "### $$\n",
    "\\left[ 1, p, p^2, \\cdots, p^{(r -1 )} \\right],\n",
    "$$\n",
    "where $r$ is the order of the RCDF matrix representation (the number of rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7180c06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.   , 0.2  , 0.04 , 0.008])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.2\n",
    "order = matrix.shape[1]\n",
    "parameter_vector = np.zeros(order)\n",
    "for i in range(order):\n",
    "    parameter_vector[i] += p**i\n",
    "parameter_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81fd4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27917, 23)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors = np.matmul(parameter_vector, matrix)\n",
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf33bb",
   "metadata": {},
   "source": [
    "# Converting the dataset into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b112c2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27917, 23]), torch.Size([27917, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.from_numpy(feature_vectors)\n",
    "targets = torch.from_numpy(target_closeness)\n",
    "features.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f45f034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2334],\n",
       "        [0.2231],\n",
       "        [0.2180],\n",
       "        ...,\n",
       "        [0.2319],\n",
       "        [0.2520],\n",
       "        [0.1761]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activating the automatic gradient \n",
    "features.requires_grad_(True)\n",
    "targets.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab54f2",
   "metadata": {},
   "source": [
    "# Dividing dataset into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68b3e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 23]),\n",
       " torch.Size([17917, 23]),\n",
       " torch.Size([10000, 1]),\n",
       " torch.Size([17917, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling and dividing the indecies\n",
    "n_samples = features.shape[0]\n",
    "n_test = 17917\n",
    "shuffled_ind = torch.randperm(n_samples)\n",
    "train_ind = shuffled_ind[:-n_test]\n",
    "test_ind = shuffled_ind[-n_test:]\n",
    "# Dividing features and targets into tain and test sets\n",
    "train_features = features[train_ind]\n",
    "test_features = features[test_ind]\n",
    "train_targets = targets[train_ind]\n",
    "test_targets = targets[test_ind]\n",
    "train_features.shape, test_features.shape, train_targets.shape, test_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b1796",
   "metadata": {},
   "source": [
    "## A function for dividing train data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628bf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing train_features and train_targets into batches\n",
    "def next_batch(train_features, train_targets, batch_size=100):\n",
    "    num_features = train_features.shape[0]\n",
    "    # Shuffling\n",
    "    shuffled_ind = torch.randperm(num_features)\n",
    "    shuffled_train_features = train_features[shuffled_ind]\n",
    "    shuffled_train_targets = train_targets[shuffled_ind]\n",
    "    # Dividing\n",
    "    i = 0\n",
    "    while i < num_features:\n",
    "        i += batch_size\n",
    "        yield (shuffled_train_features[i-batch_size:i], shuffled_train_targets[i-batch_size:i])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c88603",
   "metadata": {},
   "source": [
    "## The feedforward neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af923c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Feedforward Neural Network \n",
    "class FFNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_features = features.shape[1]\n",
    "        self.fc1 = nn.Linear(num_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = torch.tanh(self.fc1(X))\n",
    "        X = self.dropout1(X)\n",
    "        X = torch.relu(self.fc2(X))\n",
    "        return self.fc3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3199f544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN_model(\n",
       "  (fc1): Linear(in_features=23, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiation of the model\n",
    "torch.manual_seed(42)\n",
    "model = FFNN_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac6a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "64\n",
      "512\n",
      "8\n",
      "8\n",
      "1\n",
      "Number of all parameters: 2065\n"
     ]
    }
   ],
   "source": [
    "num_para = 0\n",
    "for param in model.parameters():\n",
    "    print(param.numel())\n",
    "    num_para += param.numel()\n",
    "print(f'Number of all parameters: {num_para}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20769e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaa75b",
   "metadata": {},
   "source": [
    "## Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827b7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs=1000,\n",
    "                  batch_size=100,\n",
    "                  optimizer=optimizer, \n",
    "                  model=model, \n",
    "                  loss_fn=criterion, \n",
    "                  train_features=train_features, \n",
    "                  test_features=test_features, \n",
    "                  train_targets=train_targets, \n",
    "                  test_targets=test_targets):\n",
    "    num_features = train_features.shape[0]\n",
    "    start_time = time.time()\n",
    "    all_train_loss, all_test_loss = np.zeros(n_epochs), np.zeros(n_epochs)\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        # Training: \n",
    "        epoch_losses = []\n",
    "        # looping through batches\n",
    "        for train_features, train_targets in next_batch(train_features=train_features, \n",
    "                                                        train_targets=train_targets, batch_size=batch_size): \n",
    "            train_preds = model(train_features.float())\n",
    "            train_loss = loss_fn(train_targets.float(), train_preds.float())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(train_loss.item())\n",
    "        average_epoch_loss = sum(epoch_losses)/len(epoch_losses)\n",
    "        \n",
    "        # Test:\n",
    "        with torch.no_grad():\n",
    "            test_preds = model(test_features.float())\n",
    "            test_loss = loss_fn(test_targets.float(), test_preds.float())\n",
    "        \n",
    "        all_train_loss[epoch - 1] = average_epoch_loss\n",
    "        all_test_loss[epoch - 1] = test_loss.item()\n",
    "        # Printing the result: \n",
    "        if epoch == 1 or epoch % 100 == 0:\n",
    "            print(f\"EPOCH: {epoch:{7}}\")\n",
    "            print(f\"MEAN TRAIN LOSS:   {average_epoch_loss:.11f},    TEST LOSS:   {test_loss.item():.11f}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "    print(\"The total time = \", np.round(time.time() - start_time, 3), \" seconds!\")\n",
    "    return all_train_loss, all_test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b091380",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e755479",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:       1\n",
      "MEAN TRAIN LOSS:   0.01745167866,    TEST LOSS:   0.00815720949\n",
      "-----------------------------------------\n",
      "EPOCH:     100\n",
      "MEAN TRAIN LOSS:   0.00074113562,    TEST LOSS:   0.00079576013\n",
      "-----------------------------------------\n",
      "EPOCH:     200\n",
      "MEAN TRAIN LOSS:   0.00051520998,    TEST LOSS:   0.00061267440\n",
      "-----------------------------------------\n",
      "EPOCH:     300\n",
      "MEAN TRAIN LOSS:   0.00049546565,    TEST LOSS:   0.00051992515\n",
      "-----------------------------------------\n",
      "EPOCH:     400\n",
      "MEAN TRAIN LOSS:   0.00044007151,    TEST LOSS:   0.00047623357\n",
      "-----------------------------------------\n",
      "EPOCH:     500\n",
      "MEAN TRAIN LOSS:   0.00043135762,    TEST LOSS:   0.00044308353\n",
      "-----------------------------------------\n",
      "EPOCH:     600\n",
      "MEAN TRAIN LOSS:   0.00037361428,    TEST LOSS:   0.00042099843\n",
      "-----------------------------------------\n",
      "EPOCH:     700\n",
      "MEAN TRAIN LOSS:   0.00034420218,    TEST LOSS:   0.00038873250\n",
      "-----------------------------------------\n",
      "EPOCH:     800\n",
      "MEAN TRAIN LOSS:   0.00033478078,    TEST LOSS:   0.00036591117\n",
      "-----------------------------------------\n",
      "EPOCH:     900\n",
      "MEAN TRAIN LOSS:   0.00027735476,    TEST LOSS:   0.00034967298\n",
      "-----------------------------------------\n",
      "EPOCH:    1000\n",
      "MEAN TRAIN LOSS:   0.00026962324,    TEST LOSS:   0.00032518950\n",
      "-----------------------------------------\n",
      "EPOCH:    1100\n",
      "MEAN TRAIN LOSS:   0.00025357067,    TEST LOSS:   0.00030621892\n",
      "-----------------------------------------\n",
      "EPOCH:    1200\n",
      "MEAN TRAIN LOSS:   0.00029357357,    TEST LOSS:   0.00028181289\n",
      "-----------------------------------------\n",
      "EPOCH:    1300\n",
      "MEAN TRAIN LOSS:   0.00022407442,    TEST LOSS:   0.00026287581\n",
      "-----------------------------------------\n",
      "EPOCH:    1400\n",
      "MEAN TRAIN LOSS:   0.00022997987,    TEST LOSS:   0.00025503867\n",
      "-----------------------------------------\n",
      "EPOCH:    1500\n",
      "MEAN TRAIN LOSS:   0.00018661070,    TEST LOSS:   0.00023704307\n",
      "-----------------------------------------\n",
      "EPOCH:    1600\n",
      "MEAN TRAIN LOSS:   0.00021905552,    TEST LOSS:   0.00022521387\n",
      "-----------------------------------------\n",
      "EPOCH:    1700\n",
      "MEAN TRAIN LOSS:   0.00017901795,    TEST LOSS:   0.00022133821\n",
      "-----------------------------------------\n",
      "EPOCH:    1800\n",
      "MEAN TRAIN LOSS:   0.00017036077,    TEST LOSS:   0.00021806256\n",
      "-----------------------------------------\n",
      "EPOCH:    1900\n",
      "MEAN TRAIN LOSS:   0.00017239586,    TEST LOSS:   0.00020952364\n",
      "-----------------------------------------\n",
      "EPOCH:    2000\n",
      "MEAN TRAIN LOSS:   0.00016291159,    TEST LOSS:   0.00019372473\n",
      "-----------------------------------------\n",
      "The total time =  161.956  seconds!\n"
     ]
    }
   ],
   "source": [
    "losses = training_loop(n_epochs=2000,\n",
    "                  batch_size=400,\n",
    "                  optimizer=optimizer, \n",
    "                  model=model, \n",
    "                  loss_fn=criterion, \n",
    "                  train_features=train_features, \n",
    "                  test_features=test_features, \n",
    "                  train_targets=train_targets, \n",
    "                  test_targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fc2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the losses as a numpy file\n",
    "np.save(\"fb_media_pages_train_losses_rndfc_p2.npy\", losses[0], allow_pickle=False, fix_imports=True)\n",
    "np.save(\"fb_media_pages_test_losses_rndfc_p2.npy\", losses[1], allow_pickle=False, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8b4ccf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 0.20350524208493176,    prediction: 0.2107393741607666\n",
      "index 0:       inaccuracy: 3.555%\n",
      "-----------------------------------\n",
      "target 0.20714169129715307,    prediction: 0.21578356623649597\n",
      "index 500:       inaccuracy: 4.172%\n",
      "-----------------------------------\n",
      "target 0.255845286978798,    prediction: 0.26412513852119446\n",
      "index 1000:       inaccuracy: 3.236%\n",
      "-----------------------------------\n",
      "target 0.20748504133364368,    prediction: 0.20317301154136658\n",
      "index 1500:       inaccuracy: 2.078%\n",
      "-----------------------------------\n",
      "target 0.2361310846077601,    prediction: 0.24798919260501862\n",
      "index 2000:       inaccuracy: 5.022%\n",
      "-----------------------------------\n",
      "target 0.23844465165949202,    prediction: 0.23957327008247375\n",
      "index 2500:       inaccuracy: 0.473%\n",
      "-----------------------------------\n",
      "target 0.24983889477858506,    prediction: 0.24687033891677856\n",
      "index 3000:       inaccuracy: 1.188%\n",
      "-----------------------------------\n",
      "target 0.23110485603197212,    prediction: 0.2383469194173813\n",
      "index 3500:       inaccuracy: 3.134%\n",
      "-----------------------------------\n",
      "target 0.2105032957469628,    prediction: 0.2094205915927887\n",
      "index 4000:       inaccuracy: 0.514%\n",
      "-----------------------------------\n",
      "target 0.2610541676251473,    prediction: 0.2640435993671417\n",
      "index 4500:       inaccuracy: 1.145%\n",
      "-----------------------------------\n",
      "target 0.250277947457427,    prediction: 0.25872552394866943\n",
      "index 5000:       inaccuracy: 3.375%\n",
      "-----------------------------------\n",
      "target 0.2392724283470346,    prediction: 0.2395269125699997\n",
      "index 5500:       inaccuracy: 0.106%\n",
      "-----------------------------------\n",
      "target 0.24732421352975043,    prediction: 0.2606334686279297\n",
      "index 6000:       inaccuracy: 5.381%\n",
      "-----------------------------------\n",
      "target 0.2542606005904427,    prediction: 0.261600524187088\n",
      "index 6500:       inaccuracy: 2.887%\n",
      "-----------------------------------\n",
      "target 0.18576391223444927,    prediction: 0.19094902276992798\n",
      "index 7000:       inaccuracy: 2.791%\n",
      "-----------------------------------\n",
      "target 0.23734376359001574,    prediction: 0.24056516587734222\n",
      "index 7500:       inaccuracy: 1.357%\n",
      "-----------------------------------\n",
      "target 0.22087879943203076,    prediction: 0.2167130708694458\n",
      "index 8000:       inaccuracy: 1.886%\n",
      "-----------------------------------\n",
      "target 0.22818791975591848,    prediction: 0.23735350370407104\n",
      "index 8500:       inaccuracy: 4.017%\n",
      "-----------------------------------\n",
      "target 0.2598318924316236,    prediction: 0.2597777247428894\n",
      "index 9000:       inaccuracy: 0.021%\n",
      "-----------------------------------\n",
      "target 0.22165402815596735,    prediction: 0.22375282645225525\n",
      "index 9500:       inaccuracy: 0.947%\n",
      "-----------------------------------\n",
      "target 0.20990179444318763,    prediction: 0.2140720784664154\n",
      "index 10000:       inaccuracy: 1.987%\n",
      "-----------------------------------\n",
      "target 0.2569356237534448,    prediction: 0.26216718554496765\n",
      "index 10500:       inaccuracy: 2.036%\n",
      "-----------------------------------\n",
      "target 0.2558617026510271,    prediction: 0.2667555809020996\n",
      "index 11000:       inaccuracy: 4.258%\n",
      "-----------------------------------\n",
      "target 0.20335846745945252,    prediction: 0.20899736881256104\n",
      "index 11500:       inaccuracy: 2.773%\n",
      "-----------------------------------\n",
      "target 0.23901632917894758,    prediction: 0.24333573877811432\n",
      "index 12000:       inaccuracy: 1.807%\n",
      "-----------------------------------\n",
      "target 0.24723002148513157,    prediction: 0.258530855178833\n",
      "index 12500:       inaccuracy: 4.571%\n",
      "-----------------------------------\n",
      "target 0.16428890950291775,    prediction: 0.1634734570980072\n",
      "index 13000:       inaccuracy: 0.496%\n",
      "-----------------------------------\n",
      "target 0.2664541197184234,    prediction: 0.2615561783313751\n",
      "index 13500:       inaccuracy: 1.838%\n",
      "-----------------------------------\n",
      "target 0.1800392149055865,    prediction: 0.18901965022087097\n",
      "index 14000:       inaccuracy: 4.988%\n",
      "-----------------------------------\n",
      "target 0.2558499769559201,    prediction: 0.26391226053237915\n",
      "index 14500:       inaccuracy: 3.151%\n",
      "-----------------------------------\n",
      "target 0.2089121066026655,    prediction: 0.20391225814819336\n",
      "index 15000:       inaccuracy: 2.393%\n",
      "-----------------------------------\n",
      "target 0.24554890953397038,    prediction: 0.24725987017154694\n",
      "index 15500:       inaccuracy: 0.697%\n",
      "-----------------------------------\n",
      "target 0.16527134741572871,    prediction: 0.1643024981021881\n",
      "index 16000:       inaccuracy: 0.586%\n",
      "-----------------------------------\n",
      "target 0.26622032784776545,    prediction: 0.2676798403263092\n",
      "index 16500:       inaccuracy: 0.548%\n",
      "-----------------------------------\n",
      "target 0.2720238562779854,    prediction: 0.2564009130001068\n",
      "index 17000:       inaccuracy: 5.743%\n",
      "-----------------------------------\n",
      "target 0.31799229951267627,    prediction: 0.27016666531562805\n",
      "index 17500:       inaccuracy: 15.04%\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "errors = []\n",
    "for i in range(n_test):\n",
    "    targ = test_targets[i].item()\n",
    "    feat = test_features[i].float().view(1,1,23)\n",
    "    pred = model(feat).item()\n",
    "    if targ == 0:\n",
    "        inaccuracy = 0\n",
    "    else:\n",
    "        inaccuracy = abs(1 - pred/targ) * 100\n",
    "    errors.append(inaccuracy)\n",
    "    if i%500 == 0:\n",
    "        print(f\"target {targ},    prediction: {pred}\\nindex {i}:       inaccuracy: {np.round(inaccuracy, 3)}%\")\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c08874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inaccuracy:  3.471\n"
     ]
    }
   ],
   "source": [
    "print(\"Average inaccuracy: \", np.round(sum(errors)/len(errors), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0de52ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"trained_model_closeness_fb_media_rndfc_p2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ccb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30123af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
